{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yPJL_ll8sFOq"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"go_emotions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTORry--tEqe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_train = pd.DataFrame(dataset[\"train\"])\n",
        "df_val = pd.DataFrame(dataset[\"validation\"])\n",
        "df_test = pd.DataFrame(dataset[\"test\"])\n",
        "labels = dataset[\"train\"].features[\"labels\"].feature.names\n",
        "\n",
        "ekman_map = {\n",
        "    \"anger\": [\"anger\", \"annoyance\", \"disapproval\"],\n",
        "    \"disgust\": [\"disgust\"],\n",
        "    \"fear\": [\"fear\", \"nervousness\"],\n",
        "    \"joy\": [\"joy\", \"amusement\", \"approval\", \"excitement\", \"gratitude\",\n",
        "            \"love\", \"optimism\", \"relief\", \"pride\", \"admiration\", \"desire\", \"caring\"],\n",
        "    \"sadness\": [\"sadness\", \"disappointment\", \"embarrassment\", \"grief\", \"remorse\"],\n",
        "    \"surprise\": [\"surprise\", \"realization\", \"confusion\", \"curiosity\"],\n",
        "    \"neutral\": [\"neutral\"]\n",
        "}\n",
        "\n",
        "goemotion_to_ekman = {}\n",
        "\n",
        "for group, emotion_list in ekman_map.items():\n",
        "    for emotion in emotion_list:\n",
        "        goemotion_to_ekman[emotion] = group\n",
        "\n",
        "def convert_to_ekman(row):\n",
        "    ids = row[\"labels\"]\n",
        "    if len(ids) == 0:\n",
        "        return \"neutral\"\n",
        "\n",
        "    first_id = ids[0]\n",
        "    emotion_name = labels[first_id]\n",
        "    return goemotion_to_ekman[emotion_name]\n",
        "\n",
        "\n",
        "df_train[\"ekman\"] = df_train.apply(convert_to_ekman, axis=1)\n",
        "df_val[\"ekman\"] = df_val.apply(convert_to_ekman, axis=1)\n",
        "df_test[\"ekman\"] = df_test.apply(convert_to_ekman, axis=1)\n",
        "\n",
        "ekman_classes = sorted(df_train[\"ekman\"].unique())\n",
        "\n",
        "ekman_to_id = {emotion: idx for idx, emotion in enumerate(ekman_classes)}\n",
        "\n",
        "df_train[\"label\"] = df_train[\"ekman\"].map(ekman_to_id)\n",
        "df_val[\"label\"] = df_val[\"ekman\"].map(ekman_to_id)\n",
        "df_test[\"label\"] = df_test[\"ekman\"].map(ekman_to_id)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNsxi9LjQuFs"
      },
      "outputs": [],
      "source": [
        "print(\"All text:\", df_train[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdeAuhMCEurL"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQbjsOnJtTTu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM5ID02vtWtG"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "from collections import Counter\n",
        "\n",
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    # separate punctuation from words\n",
        "    text = re.sub(r\"([.,!?;()])\", r\" \\1 \", text)\n",
        "\n",
        "    # handle contractions\n",
        "    text = re.sub(r\"n['’]t\", \" not\", text)\n",
        "    text = re.sub(r\"['’]re\", \" are\", text)\n",
        "    text = re.sub(r\"['’]s\", \" is\", text)\n",
        "    text = re.sub(r\"['’]m\", \" am\", text)\n",
        "    text = re.sub(r\"['’]ll\", \" will\", text)\n",
        "    text = re.sub(r\"['’]ve\", \" have\", text)\n",
        "    text = re.sub(r\"['’]d\", \" would\", text)\n",
        "\n",
        "    # keep only letters, numbers, and basic punctuation\n",
        "    text = re.sub(r\"[^a-z0-9.,!?;() ]+\", \" \", text)\n",
        "\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # split into tokens\n",
        "    tokens = text.split(\" \")\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "counter = Counter()\n",
        "\n",
        "for text in df_train[\"text\"]:\n",
        "    counter.update(tokenize(text))\n",
        "\n",
        "pad_token = \"<pad>\"\n",
        "unk_token = \"<unk>\"\n",
        "\n",
        "itos = []       # index to string\n",
        "stoi = {}       # string to index\n",
        "# add special tokens first\n",
        "itos.append(pad_token)\n",
        "itos.append(unk_token)\n",
        "\n",
        "stoi[pad_token] = 0\n",
        "stoi[unk_token] = 1\n",
        "\n",
        "index = 2\n",
        "\n",
        "for word, count in counter.most_common():\n",
        "    itos.append(word)\n",
        "    stoi[word] = index\n",
        "    index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duKzdFwe2L5q"
      },
      "outputs": [],
      "source": [
        "# Encoding Step\n",
        "\n",
        "import torch\n",
        "\n",
        "MAX_LEN = 30\n",
        "\n",
        "def encode(text):\n",
        "    tokens = tokenize(text)\n",
        "    ids = [stoi.get(t, stoi[\"<unk>\"]) for t in tokens]\n",
        "\n",
        "    # truncate\n",
        "    if len(ids) > MAX_LEN:\n",
        "        ids = ids[:MAX_LEN]\n",
        "\n",
        "    # pad\n",
        "    if len(ids) < MAX_LEN:\n",
        "        ids = ids + [stoi[\"<pad>\"]] * (MAX_LEN - len(ids))\n",
        "\n",
        "    return torch.tensor(ids, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGJeLeJcTk_L"
      },
      "source": [
        "GLOVE:::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x858ELbsTjVL"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -o glove.6B.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWHr8L2uTr-b"
      },
      "outputs": [],
      "source": [
        "glove_path = \"glove.6B.100d.txt\"\n",
        "\n",
        "glove = {}\n",
        "\n",
        "with open(glove_path, \"r\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = torch.tensor([float(x) for x in values[1:]], dtype=torch.float)\n",
        "        glove[word] = vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFFkqcqTTuZ9"
      },
      "outputs": [],
      "source": [
        "embed_dim = 100\n",
        "\n",
        "embedding_matrix = torch.zeros((len(itos), embed_dim))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2YtqPOETvuS"
      },
      "outputs": [],
      "source": [
        "for idx, word in enumerate(itos):\n",
        "    if word in glove:\n",
        "        embedding_matrix[idx] = glove[word]\n",
        "    else:\n",
        "        embedding_matrix[idx] = torch.randn(embed_dim) * 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGI1azEj3jKR"
      },
      "outputs": [],
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.texts = df[\"text\"].tolist()\n",
        "        self.labels = df[\"label\"].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = encode(self.texts[idx])\n",
        "        y = int(self.labels[idx])\n",
        "        return x, y\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    padded = pad_sequence(texts, batch_first=True, padding_value=0)\n",
        "    labels = torch.tensor(labels)\n",
        "    return padded, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqTWTPU93xkM"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(EmotionDataset(df_train), batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(EmotionDataset(df_val), batch_size=32, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(EmotionDataset(df_test), batch_size=32, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUm8VesY4P-Y"
      },
      "outputs": [],
      "source": [
        "class CNNEmotionClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes, dropout_rate, embedding_matrix):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.embedding.weight.data.copy_(embedding_matrix)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(embed_dim, 128, kernel_size=3)\n",
        "        self.conv4 = nn.Conv1d(embed_dim, 128, kernel_size=4)\n",
        "        self.conv5 = nn.Conv1d(embed_dim, 128, kernel_size=5)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc = nn.Linear(128  * 3, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: batch, seq_len\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "\n",
        "        c3 = self.relu(self.conv3(embedded))\n",
        "        c4 = self.relu(self.conv4(embedded))\n",
        "        c5 = self.relu(self.conv5(embedded))\n",
        "\n",
        "        # max pooling\n",
        "        p3 = torch.max(c3, dim=2).values\n",
        "        p4 = torch.max(c4, dim=2).values\n",
        "        p5 = torch.max(c5, dim=2).values\n",
        "\n",
        "        combined = torch.cat([p3, p4, p5], dim=1)\n",
        "\n",
        "        combined = self.dropout(combined)\n",
        "\n",
        "        output = self.fc(combined)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERcMTzv05mE2"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "model = CNNEmotionClassifier(\n",
        "    vocab_size=len(itos),\n",
        "    embed_dim=100,\n",
        "    num_classes=df_train[\"label\"].nunique(),\n",
        "    dropout_rate=0.3,\n",
        "    embedding_matrix=embedding_matrix\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "# The below was used for class balancing, but was not in final code due to performance (mentioned in paper)\n",
        "\n",
        "# # Applying class weights\n",
        "# class_weights = compute_class_weight(\n",
        "#     class_weight=\"balanced\",\n",
        "#     classes=np.unique(df_train[\"label\"]),\n",
        "#     y=df_train[\"label\"]\n",
        "# )\n",
        "\n",
        "# class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "model = model.to(device)\n",
        "loss_fn = loss_fn.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt82FgYK8mQ-"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "\n",
        "def train_one_epoch(model, train_loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(x)\n",
        "        loss = loss_fn(predictions, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_train_loss = total_loss / len(train_loader)\n",
        "    return average_train_loss\n",
        "\n",
        "def validate_one_epoch(model, val_loader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            predictions = model(x)\n",
        "            loss = loss_fn(predictions, y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    average_val_loss = total_loss / len(val_loader)\n",
        "    return average_val_loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P2GqPkV84E0"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "patience = 2\n",
        "patience_counter = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch\", epoch + 1)\n",
        "\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn)\n",
        "    val_loss = validate_one_epoch(model, val_loader, loss_fn)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(\"Train loss:\", train_loss)\n",
        "    print(\"Validation loss:\", val_loss)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        print(\"Validation improved, saving model\")\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), \"best_cnn.pt\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(\"No improvement. Patience =\", patience_counter)\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping activated\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_cnn.pt\"))\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Uc8xkNu9vsg"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        outputs = model(x)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_true.extend(y.cpu().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U35DMjie_ApR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "\n",
        "# convert integer predictions to emotion names\n",
        "id_to_ekman = {v: k for k, v in ekman_to_id.items()}\n",
        "\n",
        "true_names = [id_to_ekman[i] for i in all_true]\n",
        "pred_names = [id_to_ekman[i] for i in all_preds]\n",
        "\n",
        "# metrics\n",
        "accuracy = accuracy_score(true_names, pred_names)\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    true_names,\n",
        "    pred_names,\n",
        "    average=\"macro\"\n",
        ")\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Macro Precision:\", precision)\n",
        "print(\"Macro Recall:\", recall)\n",
        "print(\"Macro F1:\", f1)\n",
        "print()\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(true_names, pred_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5GNkVjbGWxA"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#  sorted list\n",
        "ekman_labels_sorted = sorted(ekman_to_id.keys())\n",
        "\n",
        "#  confusion matrix\n",
        "cm = confusion_matrix(true_names, pred_names, labels=ekman_labels_sorted)\n",
        "\n",
        "# plot\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\",\n",
        "    xticklabels=ekman_labels_sorted,\n",
        "    yticklabels=ekman_labels_sorted,\n",
        "    annot_kws={\"size\": 18}   # increase number size here\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Predicted Emotion\", fontsize=18)\n",
        "plt.ylabel(\"True Emotion\", fontsize=18)\n",
        "plt.title(\"CNN Confusion Matrix, Ekman Emotion Groups\", fontsize=22)\n",
        "\n",
        "plt.xticks(rotation=45, ha=\"right\", fontsize=16)\n",
        "plt.yticks(rotation=0, fontsize=16)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgWKjcTxjHa4"
      },
      "source": [
        "# The below was used to implement the manual gridsearch of the hyperparameter tuning. The parameters were extratced once and used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HVRgZ72-On7t"
      },
      "outputs": [],
      "source": [
        "# search_space = {\n",
        "#     \"embed_dim\": [128, 200],\n",
        "#     \"num_filters\": [128, 256],\n",
        "#     \"dropout_rate\": [0.3, 0.5],\n",
        "#     \"learning_rate\": [0.001, 0.0005]\n",
        "# }\n",
        "\n",
        "# class CNNEmotionClassifier(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, num_filters, num_classes, dropout_rate):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # embedding layer\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "#         # convolution layers with variable filter count\n",
        "#         self.conv3 = nn.Conv1d(embed_dim, num_filters, kernel_size=3)\n",
        "#         self.conv4 = nn.Conv1d(embed_dim, num_filters, kernel_size=4)\n",
        "#         self.conv5 = nn.Conv1d(embed_dim, num_filters, kernel_size=5)\n",
        "\n",
        "#         # activation\n",
        "#         self.relu = nn.ReLU()\n",
        "\n",
        "#         # dropout\n",
        "#         self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "#         # fully connected output layer\n",
        "#         self.fc = nn.Linear(num_filters * 3, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         embedded = self.embedding(x)\n",
        "\n",
        "#         embedded = embedded.permute(0, 2, 1)\n",
        "\n",
        "#         c3 = self.relu(self.conv3(embedded))\n",
        "#         c4 = self.relu(self.conv4(embedded))\n",
        "#         c5 = self.relu(self.conv5(embedded))\n",
        "\n",
        "#         p3 = torch.max(c3, dim=2).values\n",
        "#         p4 = torch.max(c4, dim=2).values\n",
        "#         p5 = torch.max(c5, dim=2).values\n",
        "\n",
        "#         combined = torch.cat([p3, p4, p5], dim=1)\n",
        "\n",
        "#         combined = self.dropout(combined)\n",
        "\n",
        "#         output = self.fc(combined)\n",
        "\n",
        "#         return output\n",
        "\n",
        "\n",
        "\n",
        "# def run_experiment(embed_dim, num_filters, dropout_rate, learning_rate):\n",
        "#     model = CNNEmotionClassifier(\n",
        "#         vocab_size=len(itos),\n",
        "#         embed_dim=embed_dim,\n",
        "#         num_filters=num_filters,\n",
        "#         num_classes=df_train[\"label\"].nunique(),\n",
        "#         dropout_rate=dropout_rate\n",
        "#     ).to(device)\n",
        "\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#     loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "#     best_val = float(\"inf\")\n",
        "#     patience = 2\n",
        "#     patience_counter = 0\n",
        "\n",
        "#     for epoch in range(5):   # fewer epochs for search\n",
        "#         train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn)\n",
        "#         val_loss = validate_one_epoch(model, val_loader, loss_fn)\n",
        "\n",
        "#         if val_loss < best_val:\n",
        "#             best_val = val_loss\n",
        "#             patience_counter = 0\n",
        "#         else:\n",
        "#             patience_counter += 1\n",
        "#             if patience_counter >= patience:\n",
        "#                 break\n",
        "\n",
        "#     return best_val\n",
        "\n",
        "\n",
        "# import itertools\n",
        "\n",
        "# keys = search_space.keys()\n",
        "# values = search_space.values()\n",
        "\n",
        "# best_config = None\n",
        "# best_score = float(\"inf\")\n",
        "\n",
        "# for combo in itertools.product(*values):\n",
        "#     params = dict(zip(keys, combo))\n",
        "\n",
        "#     print(\"Testing:\", params)\n",
        "\n",
        "#     score = run_experiment(\n",
        "#         embed_dim=params[\"embed_dim\"],\n",
        "#         num_filters=params[\"num_filters\"],\n",
        "#         dropout_rate=params[\"dropout_rate\"],\n",
        "#         learning_rate=params[\"learning_rate\"]\n",
        "#     )\n",
        "\n",
        "#     print(\"Score:\", score)\n",
        "\n",
        "#     if score < best_score:\n",
        "#         best_score = score\n",
        "#         best_config = params\n",
        "\n",
        "# print(\"Best configuration found:\", best_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_UB_nUqXbrU"
      },
      "outputs": [],
      "source": [
        "# def predict(text):\n",
        "#     tokens = tokenize(text)\n",
        "#     ids = [stoi.get(t, stoi[\"<unk>\"]) for t in tokens]\n",
        "#     x = torch.tensor(ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         logits = model(x)\n",
        "#         pred = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "#     return id_to_ekman[pred]\n",
        "\n",
        "# # Ask user for input\n",
        "# user_input = input(\"Enter a sentence: \")\n",
        "# while user_input.lower() != \"exit\":\n",
        "#   emotion = predict(user_input)\n",
        "#   print(\"Predicted emotion:\", emotion)\n",
        "#   user_input = input(\"Enter a sentence (type 'exit' to quit): \")\n",
        "#   print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3bUR4NitnqnC"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFzTRSf1ntxS"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN3KNQ9hnrxo"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqI9STQBnwdh"
      },
      "outputs": [],
      "source": [
        "class BERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.texts = df[\"text\"].tolist()\n",
        "        self.labels = df[\"label\"].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoded = tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=64,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoded[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": torch.tensor(label)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LMyy4JTn0F8"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(BERTDataset(df_train), batch_size=16, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(BERTDataset(df_val), batch_size=16)\n",
        "test_loader = torch.utils.data.DataLoader(BERTDataset(df_test), batch_size=16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYzund5an1a5"
      },
      "outputs": [],
      "source": [
        "num_classes = df_train[\"label\"].nunique()\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=num_classes\n",
        ")\n",
        "\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rl3h9m9n2ra"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrjSBd0Bn4Lk"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXL_n1syoDL7"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            total_loss += outputs.loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "476i0fNOoE7t"
      },
      "outputs": [],
      "source": [
        "best_val = float(\"inf\")\n",
        "patience = 2\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(5):\n",
        "    print(\"Epoch\", epoch + 1)\n",
        "\n",
        "    train_loss = train_one_epoch(model, train_loader)\n",
        "    val_loss = evaluate(model, val_loader)\n",
        "\n",
        "    print(\"Train loss:\", train_loss)\n",
        "    print(\"Val loss:\", val_loss)\n",
        "\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), \"best_bert.pt\")\n",
        "        print(\"Model improved, saving\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(\"No improvement, patience:\", patience_counter)\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Stopping early\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_evKKBbKoGaL"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"best_bert.pt\"))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_true.extend(labels.cpu().tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obPLhqh7oLe9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "true_names = [id_to_ekman[i] for i in all_true]\n",
        "pred_names = [id_to_ekman[i] for i in all_preds]\n",
        "\n",
        "print(classification_report(true_names, pred_names))\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    true_names,\n",
        "    pred_names,\n",
        "    average=\"macro\"\n",
        ")\n",
        "\n",
        "print(\"Macro Precision:\", precision)\n",
        "print(\"Macro Recall:\", recall)\n",
        "print(\"Macro F1:\", f1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdDLTyn-qjDh"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(true_names, pred_names, labels=ekman_labels_sorted)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\",\n",
        "    xticklabels=ekman_labels_sorted,\n",
        "    yticklabels=ekman_labels_sorted,\n",
        "    annot_kws={\"size\": 18}\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Predicted Emotion\", fontsize=16)\n",
        "plt.ylabel(\"True Emotion\", fontsize=16)\n",
        "plt.title(\"BERT Confusion Matrix, Ekman Emotion Groups\", fontsize=18)\n",
        "plt.xticks(rotation=45, ha=\"right\", fontsize=16)\n",
        "plt.yticks(rotation=0, fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jrzECRd1tkFe"
      },
      "outputs": [],
      "source": [
        "def predict_bert(text):\n",
        "    encoded = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=64,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids = encoded[\"input_ids\"].to(device)\n",
        "    attention_mask = encoded[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pred_id = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "    return id_to_ekman[pred_id]\n",
        "while True:\n",
        "    user_input = input(\"Enter a sentence (or 'exit'): \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "    print(\"Predicted emotion:\", predict_bert(user_input))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKnxGYKURYWd"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "df_train[\"svm_ekman\"] = df_train[\"ekman\"].apply(lambda x: [x])\n",
        "df_val[\"svm_ekman\"] = df_val[\"ekman\"].apply(lambda x: [x])\n",
        "df_test[\"svm_ekman\"] = df_test[\"ekman\"].apply(lambda x: [x])\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train = mlb.fit_transform(df_train[\"svm_ekman\"])\n",
        "y_val = mlb.transform(df_val[\"svm_ekman\"])\n",
        "y_test = mlb.transform(df_test[\"svm_ekman\"])\n",
        "\n",
        "X_train = df_train[\"text\"]\n",
        "X_val = df_val[\"text\"]\n",
        "X_test = df_test[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c6lnv05RbxE"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    (\"features\", FeatureUnion([\n",
        "        (\"word\", TfidfVectorizer(\n",
        "            analyzer=\"word\",\n",
        "            sublinear_tf=True\n",
        "        )),\n",
        "        (\"char\", TfidfVectorizer(\n",
        "            analyzer=\"char\",\n",
        "            sublinear_tf=True\n",
        "        ))\n",
        "    ])),\n",
        "    (\"svm\", OneVsRestClassifier(\n",
        "        LinearSVC(class_weight=\"balanced\")\n",
        "    ))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zF8Pzw1Rm_l"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    \"features__word__ngram_range\": [(1,2), (1,3)],\n",
        "    \"features__word__max_features\": [20000, 40000],\n",
        "    \"features__char__ngram_range\": [(3,4), (3,5)],\n",
        "    \"features__char__max_features\": [10000, 20000],\n",
        "    \"svm__estimator__C\": [0.1, 0.3]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"f1_macro\",\n",
        "    cv=2,\n",
        "    verbose=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters found\")\n",
        "print(grid.best_params_)\n",
        "print(\"Best macro F1 during CV\", grid.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d_MFXUbRpuv"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "val_preds = best_model.predict(X_val)\n",
        "test_preds = best_model.predict(X_test)\n",
        "\n",
        "print(\"Validation macro F1\", f1_score(y_val, val_preds, average=\"macro\"))\n",
        "print(\"Test macro F1\", f1_score(y_test, test_preds, average=\"macro\"))\n",
        "\n",
        "print(\"Validation report\")\n",
        "print(classification_report(y_val, val_preds, target_names=mlb.classes_))\n",
        "\n",
        "print(\"Test report\")\n",
        "print(classification_report(y_test, test_preds, target_names=mlb.classes_))\n",
        "\n",
        "y_true_single = np.argmax(y_test, axis=1)\n",
        "y_pred_single = np.argmax(test_preds, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_true_single, y_pred_single)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=mlb.classes_,\n",
        "            yticklabels=mlb.classes_)\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=45)\n",
        "\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion matrix for LinearSVM\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evtS-3NQRuR_"
      },
      "outputs": [],
      "source": [
        "def predict_emotion(text):\n",
        "    pred = best_model.predict([text])[0]\n",
        "    label = mlb.inverse_transform(pred.reshape(1, -1))[0]\n",
        "    return label\n",
        "\n",
        "def emotion_confidences(text):\n",
        "    scores = best_model.decision_function([text])[0]\n",
        "    label_scores = dict(zip(mlb.classes_, scores))\n",
        "    return label_scores\n",
        "\n",
        "def top3_emotions(text):\n",
        "    scores = best_model.decision_function([text])[0]\n",
        "    label_scores = list(zip(mlb.classes_, scores))\n",
        "    label_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    return label_scores[:3]\n",
        "\n",
        "def print_prediction(text):\n",
        "    print(\"Input text:\", text)\n",
        "    print(\"Predicted primary emotion:\", predict_emotion(text))\n",
        "    print(\"\\nTop three predicted emotions:\")\n",
        "    for label, score in top3_emotions(text):\n",
        "        print(f\"{label}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2twigkD-T1b_"
      },
      "outputs": [],
      "source": [
        "print_prediction(\"I saw snow coming down and it made me feel very good\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}